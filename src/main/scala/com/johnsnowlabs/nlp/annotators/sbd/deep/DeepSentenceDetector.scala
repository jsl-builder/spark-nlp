package com.johnsnowlabs.nlp.annotators.sbd.deep

import com.johnsnowlabs.nlp.{Annotation, AnnotatorModel}
import com.johnsnowlabs.nlp.AnnotatorType.{CHUNK, DOCUMENT, TOKEN}
import com.johnsnowlabs.nlp.annotator.SentenceDetector
import com.johnsnowlabs.nlp.annotators.common.SentenceSplit
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.param.BooleanParam
import org.apache.spark.ml.util.Identifiable

class DeepSentenceDetector(override val uid: String) extends AnnotatorModel[DeepSentenceDetector]{

  def this() = this(Identifiable.randomUID("SENTENCE ML"))

  /** Annotator reference id. Used to identify elements in metadata or to refer to this annotator type */
  override val requiredAnnotatorTypes: Array[AnnotatorType] = Array(DOCUMENT, TOKEN, CHUNK)
  override val annotatorType: AnnotatorType = DOCUMENT

  val includeRules = new BooleanParam(this, "includeRules",
    "Includes rule-based sentence detector as first filter")

  def setIncludeRules(value: Boolean): this.type = set(includeRules, value)
  setDefault(includeRules, false)

  /**
    * takes a document and annotations and produces new annotations of this annotator's annotation type
    *
    * @param annotations Annotations that correspond to inputAnnotationCols generated by previous annotators if any
    * @return any number of annotations processed for every input annotation. Not necessary one to one relationship
    */
  override def annotate(annotations: Seq[Annotation]): Seq[Annotation] = {

    if ($(includeRules)) {

      val document = getDocument(annotations)

      val pragmaticSentenceDetector = new SentenceDetector().annotate(document)

      if (pragmaticSentenceDetector.length == 1 && pragmaticSentenceDetector.head.metadata == document.head.metadata){
        deepSentenceDetector(annotations)
      }
      else {

        pragmaticSentenceDetector

        //val tests = pragmaticSentenceDetector.map(sentence => deepSentenceDetector(Seq(sentence)))
        //Seq(Annotation(annotatorType, 0 , 0, "under construction", Map()))

      }
    } else {
      deepSentenceDetector(annotations)
    }

  }

  def getDocument(annotations: Seq[Annotation]): Seq[Annotation] = {
    annotations.filter(annotation => annotation.annotatorType == DOCUMENT)
  }

  def deepSentenceDetector(annotations: Seq[Annotation]): Seq[Annotation] = {
    val nerEntities = getNerEntities(annotations)
    val sentence = retrieveSentence(annotations)
    segmentSentence(nerEntities, sentence)
  }

  def getNerEntities(annotations: Seq[Annotation]): Seq[Annotation] = {
    annotations.filter(annotation => annotation.annotatorType == CHUNK)
  }

  def retrieveSentence(annotations: Seq[Annotation]): String = {
    val sentences = SentenceSplit.unpack(annotations)
    val sentence = sentences.map(sentence => sentence.content)
    sentence.mkString(" ")
  }

  def segmentSentence(nerEntities: Seq[Annotation], sentence: String): Seq[Annotation] = {
    var sentenceNumber = 0
    nerEntities.zipWithIndex.map{case (nerEntity, index) =>
      sentenceNumber += 1
      if (index != nerEntities.length-1){
        val beginIndex = nerEntity.begin
        val endIndex = nerEntities(index+1).begin-1
        val segmentedSentence = sentence.substring(beginIndex, endIndex)
        Annotation(annotatorType, 0, segmentedSentence.length-1, segmentedSentence,
                   Map("sentence" -> sentenceNumber.toString))
      } else {
        val beginIndex = nerEntity.begin
        val segmentedSentence = sentence.substring(beginIndex)
        Annotation(annotatorType, 0, segmentedSentence.length-1, segmentedSentence,
          Map("sentence" -> sentenceNumber.toString))
      }
    }
  }

}
