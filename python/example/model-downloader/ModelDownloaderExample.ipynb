{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary imports, start spark and create our model downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) \n",
      "[GCC 7.2.0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "print(sys.version)\n",
    "\n",
    "from sparknlp.pretrained import ResourceDownloader\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ner\")\\\n",
    "    .master(\"local[1]\")\\\n",
    "    .config(\"spark.driver.memory\",\"4G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\")\\\n",
    "    .config(\"spark.jar\", \"lib/sparknlp.jar\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"500m\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# instantiate the downloader\n",
    "downloader = ResourceDownloader()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dummy spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some mock data to play with\n",
    "l = [\n",
    "  (1,'To be or not to be'),\n",
    "  (2,'This is it!')\n",
    "]\n",
    "\n",
    "data = spark.createDataFrame(l, ['docID','text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we intend to download a POS model by its name and language, which requires tokenized text. Hence, we create our tokenizer pipeline to get the data ready.\n",
    "Then, we add the POS along the other annotators and transform some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|docID|              text|            document|            sentence|               token|                 pos|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    1|To be or not to be|[[document, 0, 17...|[[document, 0, 17...|[[token, 0, 1, To...|[[pos, 0, 1, TO, ...|\n",
      "|    2|       This is it!|[[document, 0, 10...|[[document, 0, 10...|[[token, 0, 3, Th...|[[pos, 0, 3, DT, ...|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download directly - models\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\")\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "    \n",
    "# pos tagger\n",
    "pos = PerceptronModel.pretrained() \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"pos\")\n",
    "    \n",
    "pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, pos])\n",
    "\n",
    "output = pipeline.fit(data).transform(data)\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader.clearCache(\"pos_fast\", \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use predefined BasicPipeline in order to annotate a dataframe with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|docID|              text|            document|               token|              normal|               lemma|                 pos|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    1|To be or not to be|[[document, 0, 17...|[[token, 0, 1, To...|[[token, 0, 1, To...|[[token, 0, 1, To...|[[pos, 0, 1, TO, ...|\n",
      "|    2|       This is it!|[[document, 0, 10...|[[token, 0, 3, Th...|[[token, 0, 3, Th...|[[token, 0, 3, Th...|[[pos, 0, 3, DT, ...|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download predefined - pipelines\n",
    "from sparknlp.pretrained.pipeline.en import BasicPipeline\n",
    "\n",
    "basic_data = BasicPipeline.annotate(data, \"text\")\n",
    "basic_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also annotate a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lemma': ['This',\n",
       "  'world',\n",
       "  'be',\n",
       "  'make',\n",
       "  'up',\n",
       "  'of',\n",
       "  'good',\n",
       "  'and',\n",
       "  'bad',\n",
       "  'thing'],\n",
       " 'document': ['This world is made up of good and bad things'],\n",
       " 'normal': ['This',\n",
       "  'world',\n",
       "  'is',\n",
       "  'made',\n",
       "  'up',\n",
       "  'of',\n",
       "  'good',\n",
       "  'and',\n",
       "  'bad',\n",
       "  'things'],\n",
       " 'pos': ['DT', 'NN', 'VBZ', 'VBN', 'RP', 'IN', 'JJ', 'CC', 'JJ', 'NNS'],\n",
       " 'token': ['This',\n",
       "  'world',\n",
       "  'is',\n",
       "  'made',\n",
       "  'up',\n",
       "  'of',\n",
       "  'good',\n",
       "  'and',\n",
       "  'bad',\n",
       "  'things']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# annotat quickly from string\n",
    "BasicPipeline().annotate(\"This world is made up of good and bad things\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, here we download a Pipeline by its name and language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test clearCache\n",
    "downloader.clearCache(\"pipeline_basic\", \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clear cache of recently downloaded pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|docID|              text|            document|               token|              normal|               lemma|                 pos|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    1|To be or not to be|[[document, 0, 17...|[[token, 0, 1, To...|[[token, 0, 1, To...|[[token, 0, 1, To...|[[pos, 0, 1, TO, ...|\n",
      "|    2|       This is it!|[[document, 0, 10...|[[token, 0, 3, Th...|[[token, 0, 3, Th...|[[token, 0, 3, Th...|[[pos, 0, 3, DT, ...|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download directly - pipeline models\n",
    "\n",
    "# simple pipeline with document assembler and tokenizer\n",
    "pipeline = downloader.downloadPipeline(\"pipeline_basic\", \"en\")\n",
    "pipeline.transform(data).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we proceed to download a POS model, utilizing the downloader alternative way to retrieve it.\n",
    "We do the right way for the NER model though.\n",
    "Then, we retrieve the Basic Pipeline and combine these models to use them appropriately meeting their requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|docID|              text|            document|               token|              normal|               lemma|                 pos|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    1|To be or not to be|[[document, 0, 17...|[[token, 0, 1, To...|[[token, 0, 1, To...|[[token, 0, 1, To...|[[pos, 0, 1, TO, ...|\n",
      "|    2|       This is it!|[[document, 0, 10...|[[token, 0, 3, Th...|[[token, 0, 3, Th...|[[token, 0, 3, Th...|[[pos, 0, 3, DT, ...|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|docID|              text|            document|               token|              normal|               lemma|                 pos|                 ner|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    1|To be or not to be|[[document, 0, 17...|[[token, 0, 1, To...|[[token, 0, 1, To...|[[token, 0, 1, To...|[[pos, 0, 1, TO, ...|[[named_entity, 0...|\n",
      "|    2|       This is it!|[[document, 0, 10...|[[token, 0, 3, Th...|[[token, 0, 3, Th...|[[token, 0, 3, Th...|[[pos, 0, 3, DT, ...|[[named_entity, 0...|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download predefined - models\n",
    "\n",
    "pos = downloader.downloadModel(PerceptronModel, \"pos_fast\", \"en\")    \n",
    "pos.setInputCols([\"document\", \"normal\"]).setOutputCol(\"pos\")\n",
    "\n",
    "ner = NerCrfModel.pretrained()\n",
    "ner.setInputCols([\"pos\", \"normal\", \"document\"]).setOutputCol(\"ner\")\n",
    "\n",
    "annotation_pipeline = BasicPipeline.pretrained()\n",
    "annotation_data = annotation_pipeline.transform(data)\n",
    "annotation_data.show()\n",
    "\n",
    "pos_tagged = pos.transform(annotation_data)\n",
    "ner_tagged = ner.transform(pos_tagged)\n",
    "ner_tagged.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try a sentiment analysis pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': ['This is a good movie!!!'],\n",
       " 'token': ['This', 'is', 'a', 'good', 'movie', '!', '!!'],\n",
       " 'normal': ['This', 'is', 'a', 'good', 'movie'],\n",
       " 'sentiment': ['positive']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sparknlp.pretrained.pipeline.en import SentimentPipeline\n",
    "\n",
    "SentimentPipeline.annotate(\"This is a good movie!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
