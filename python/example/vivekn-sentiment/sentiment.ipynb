{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nlp.johnsnowlabs.com/assets/images/logo.png\" width=\"180\" height=\"50\" style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vivekn Sentiment Analysis\n",
    "\n",
    "In the following example, we walk-through Sentiment Analysis training and prediction using Spark NLP Annotators.\n",
    "\n",
    "The ViveknSentimentApproach annotator will compute [Vivek Narayanan algorithm](https://arxiv.org/pdf/1305.6143.pdf) with either a column in training dataset with rows labelled 'positive' or 'negative' or a folder full of positive text and a folder with negative text. Using n-grams and negation of sequences, this statistical model can achieve high accuracy if trained properly.\n",
    "\n",
    "Spark can be leveraged in training by utilizing ReadAs.Dataset setting. Spark will be used during prediction by default.\n",
    "\n",
    "We also include in this pipeline a spell checker which shall correct our sentences for better Sentiment Analysis accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Call necessary imports and set the resource path to read local data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "#sys.path.append('../../')\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "\n",
    "#Setting location of resource Directory\n",
    "resource_path= \"../../../src/test/resources/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load SparkSession if not already there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"VivekNarayanSentimentApproach\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\",\"8G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\")\\\n",
    "    .config(\"spark.jar\", \"lib/sparknlp.jar\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"500m\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 3. Load a spark dataset and put it in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+\n",
      "|itemid|sentiment|                text|\n",
      "+------+---------+--------------------+\n",
      "|     1|        0|                 ...|\n",
      "|     2|        0|                 ...|\n",
      "|     3|        1|              omg...|\n",
      "|     4|        0|          .. Omga...|\n",
      "|     5|        0|         i think ...|\n",
      "|     6|        0|         or i jus...|\n",
      "|     7|        1|       Juuuuuuuuu...|\n",
      "|     8|        0|       Sunny Agai...|\n",
      "|     9|        1|      handed in m...|\n",
      "|    10|        1|      hmmmm.... i...|\n",
      "|    11|        0|      I must thin...|\n",
      "|    12|        1|      thanks to a...|\n",
      "|    13|        0|      this weeken...|\n",
      "|    14|        0|     jb isnt show...|\n",
      "|    15|        0|     ok thats it ...|\n",
      "|    16|        0|    &lt;-------- ...|\n",
      "|    17|        0|    awhhe man.......|\n",
      "|    18|        1|    Feeling stran...|\n",
      "|    19|        0|    HUGE roll of ...|\n",
      "|    20|        0|    I just cut my...|\n",
      "+------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load the input data to be annotated\n",
    "data = spark. \\\n",
    "        read. \\\n",
    "        parquet( resource_path+\"sentiment.parquet\"). \\\n",
    "        limit(1000).cache()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Create the document assembler, which will put target text column into Annotation form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the dataframe\n",
    "document_assembler = DocumentAssembler() \\\n",
    "            .setInputCol(\"text\")\\\n",
    "            .setOutputCol(\"document\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+--------------------+\n",
      "|itemid|sentiment|                text|            document|\n",
      "+------+---------+--------------------+--------------------+\n",
      "|     1|        0|                 ...|[[document, 0, 39...|\n",
      "|     2|        0|                 ...|[[document, 0, 31...|\n",
      "|     3|        1|              omg...|[[document, 0, 22...|\n",
      "|     4|        0|          .. Omga...|[[document, 0, 12...|\n",
      "|     5|        0|         i think ...|[[document, 0, 37...|\n",
      "+------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Example: Checkout the output of document assembler\n",
    "assembled = document_assembler.transform(data)\n",
    "assembled.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Create Sentence detector to parse sub sentences in every document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sentence detector\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+--------------------+--------------------+\n",
      "|itemid|sentiment|                text|            document|            sentence|\n",
      "+------+---------+--------------------+--------------------+--------------------+\n",
      "|     1|        0|                 ...|[[document, 0, 39...|[[document, 0, 27...|\n",
      "|     2|        0|                 ...|[[document, 0, 31...|[[document, 0, 29...|\n",
      "|     3|        1|              omg...|[[document, 0, 22...|[[document, 0, 22...|\n",
      "|     4|        0|          .. Omga...|[[document, 0, 12...|[[document, 0, 0,...|\n",
      "|     5|        0|         i think ...|[[document, 0, 37...|[[document, 0, 33...|\n",
      "+------+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Example: Checkout the output of sentence detector\n",
    "sentence_data = sentence_detector.transform(assembled)\n",
    "sentence_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. The tokenizer will match standard tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizer\n",
    "tokenizer = Tokenizer() \\\n",
    "            .setInputCols([\"sentence\"]) \\\n",
    "            .setOutputCol(\"token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|itemid|sentiment|                text|            document|            sentence|               token|\n",
      "+------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|     1|        0|                 ...|[[document, 0, 39...|[[document, 0, 27...|[[token, 0, 1, is...|\n",
      "|     2|        0|                 ...|[[document, 0, 31...|[[document, 0, 29...|[[token, 0, 0, I,...|\n",
      "|     3|        1|              omg...|[[document, 0, 22...|[[document, 0, 22...|[[token, 0, 2, om...|\n",
      "|     4|        0|          .. Omga...|[[document, 0, 12...|[[document, 0, 0,...|[[token, 0, 0, .,...|\n",
      "|     5|        0|         i think ...|[[document, 0, 37...|[[document, 0, 33...|[[token, 0, 0, i,...|\n",
      "+------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Example: Checkout the outout of tokenizer\n",
    "tokenized = tokenizer.transform(sentence_data)\n",
    "tokenized.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Normalizer will clean out the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer() \\\n",
    "            .setInputCols([\"token\"]) \\\n",
    "            .setOutputCol(\"normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. The spell checker will correct normalized tokens, this trains with a dictionary of english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Spell Checker\n",
    "spell_checker = NorvigSweetingApproach() \\\n",
    "            .setInputCols([\"normal\"]) \\\n",
    "            .setOutputCol(\"spell\") \\\n",
    "            .setDictionary( resource_path+ \"spell/words.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Create the ViveknSentimentApproach and set resources to train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_detector = ViveknSentimentApproach() \\\n",
    "    .setInputCols([\"spell\", \"sentence\"]) \\\n",
    "    .setOutputCol(\"sentiment\") \\\n",
    "    .setPruneCorpus(0) \\\n",
    "    .setPositiveSource(resource_path+\"vivekn/positive\") \\\n",
    "    .setNegativeSource(resource_path+\"vivekn/negative\") \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. The finisher will utilize sentiment analysis output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"sentiment\"]) \\\n",
    "    .setIncludeMetadata(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. Fit and predict over data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed pipeline process: 4.109271049499512\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    normalizer,\n",
    "    spell_checker,\n",
    "    sentiment_detector,\n",
    "    finisher\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "sentiment_data = pipeline.fit(data).transform(data)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time elapsed pipeline process: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 13. Check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)... -> na@na@na@negative\n",
      "&lt;---Sad level is 3. I was writing a massive blog tweet on Myspace and my comp shut down. Now it's all lost *lays in fetal position* -> na@na@negative@na\n",
      "I didn't realize it was THAT deep. Geez give a girl a warning atleast! -> negative@na\n",
      "SOX!     Floyd was great, but relievers need a scolding! -> na@negative\n",
      "i was too slow to get $1 Up tix -> negative\n"
     ]
    }
   ],
   "source": [
    "# Negative Sentiments\n",
    "for r in sentiment_data.filter(sentiment_data.finished_sentiment.contains('negative')).take(5):\n",
    "    print(r['text'].strip(),\"->\",r['finished_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't be bothered. i wish i could spend the rest of my life just sat here and going to gigs. seriously. -> na@positive@na\n",
      "Feeeling like shit right now. I really want to sleep, but nooo I have 3 hours of dancing and an art assignment to finish. -> na@positive\n",
      "i miss you guys too     i think i'm wearing skinny jeans a cute sweater and heels   not really sure   what are you doing today -> positive\n",
      "(: !!!!!! - so i wrote something last week. and i got a call from someone in the new york office... http://tumblr.com/xcn21w6o7 -> na@positive@na\n",
      "and the entertainment is over, someone complained properly..   @rupturerapture experimental you say? he should experiment with a melody -> positive@na@na\n"
     ]
    }
   ],
   "source": [
    "# Positive Sentiments\n",
    "for r in sentiment_data.filter(sentiment_data.finished_sentiment.contains('positive')).take(5):\n",
    "    print(r['text'].strip(),\"->\",r['finished_sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Can also be used directly on an array of dummy text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|I am happy and li...|\n",
      "|Have to say somet...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_data = spark.sparkContext.parallelize([[\"I am happy and like this spark NLP\"], [\"Have to say something bad now\"]]).toDF().toDF(\"text\")\n",
    "dummy_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|                text|finished_sentiment|\n",
      "+--------------------+------------------+\n",
      "|I am happy and li...|          positive|\n",
      "|Have to say somet...|          negative|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(dummy_data).transform(dummy_data).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. The pipeline could be saved on disk for future reuse. Either after or before fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed in write before fiting pipelines: 0.3147120475769043\n",
      "Time elapsed in write after fiting pipelines: 6.535132169723511\n"
     ]
    }
   ],
   "source": [
    "new_pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    normalizer,\n",
    "    spell_checker,\n",
    "    sentiment_detector,\n",
    "    finisher\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "new_pipeline.write().overwrite().save(\"./ps\")\n",
    "end = time.time()\n",
    "print(\"Time elapsed in write before fiting pipelines: \" + str(end - start))\n",
    "start = time.time()\n",
    "new_pipeline.fit(data).write().overwrite().save(\"./ms\")\n",
    "end = time.time()\n",
    "print(\"Time elapsed in write after fiting pipelines: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. Pipelines can be easily loaded back in memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed in read pipelines: 5.312443494796753\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "p = Pipeline.read().load(\"./ps\")\n",
    "pm = PipelineModel.read().load(\"./ms\")\n",
    "end = time.time()\n",
    "print(\"Time elapsed in read pipelines: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------------------+\n",
      "|itemid|                text|finished_sentiment|\n",
      "+------+--------------------+------------------+\n",
      "|     4|          .. Omga...| na@na@na@negative|\n",
      "|    24|   &lt;---Sad lev...| na@na@negative@na|\n",
      "|    30|   I didn't reali...|       negative@na|\n",
      "|    37|   SOX!     Floyd...|       na@negative|\n",
      "|    80|  i was too slow ...|          negative|\n",
      "|    98|  My new car was ...|       negative@na|\n",
      "|   118|  True, highly su...|    na@negative@na|\n",
      "|   135| #squarespace bri...|       negative@na|\n",
      "|   136| #Susan Boyle did...|    na@negative@na|\n",
      "|   145| &quot;I want you...|       na@negative|\n",
      "|   195|- @Darcy_Lussier ...|    na@negative@na|\n",
      "|   219| @khead I'll tell...| na@negative@na@na|\n",
      "|   238|- @raybooysen I k...|          negative|\n",
      "|   253|    Not feeling i...|          negative|\n",
      "|   273|   No man is wort...|          negative|\n",
      "|   288|        wish ella...|          negative|\n",
      "|   305|    TODAY WAS A G...|          negative|\n",
      "|   312|   fail maths  i ...|          negative|\n",
      "|   315|-   going in town...|          negative|\n",
      "|   324|  - Poor lil mous...| na@negative@na@na|\n",
      "+------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "1000\n",
      "Time elapsed in using loaded pipelines: 0.8672728538513184\n"
     ]
    }
   ],
   "source": [
    "# Using the fitted pipeline read from disk\n",
    "start = time.time()\n",
    "pm.transform(data).where(\"finished_sentiment like '%negative%'\").show()\n",
    "print(pm.transform(data).count())\n",
    "end = time.time()\n",
    "print(\"Time elapsed in using loaded pipelines: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
