{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nlp.johnsnowlabs.com/assets/images/logo.png\" width=\"180\" height=\"50\" style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning NER\n",
    "\n",
    "In the following example, we walk-through a LSTM NER model training and prediction. This annotator is implemented on top of TensorFlow.\n",
    "\n",
    "This annotator will take a series of word embedding vectors, training CoNLL dataset, plus a validation dataset. We include our own predefined Tensorflow Graphs, but it will train all layers during fit() stage.\n",
    "\n",
    "DL NER will compute several layers of BI-LSTM in order to auto generate entity extraction, and it will leverage batch-based distributed calls to native TensorFlow libraries during prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark `2.4` and Spark NLP `1.8.2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Call necessary imports and set the resource folder path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "import time\n",
    "import zipfile\n",
    "#Setting location of resource Directory\n",
    "resource_path= \"../../../src/test/resources/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Download CoNLL 2003 data if not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CoNLL 2003 Dataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "url = \"https://github.com/patverga/torch-ner-nlp-from-scratch/raw/master/data/conll2003/\"\n",
    "file_train=\"eng.train\"\n",
    "file_testa= \"eng.testa\"\n",
    "file_testb= \"eng.testb\"\n",
    "# https://github.com/patverga/torch-ner-nlp-from-scratch/tree/master/data/conll2003\n",
    "if not Path(file_train).is_file():   \n",
    "    print(\"Downloading \"+file_train)\n",
    "    urllib.request.urlretrieve(url+file_train, file_train)\n",
    "if not Path(file_testa).is_file():\n",
    "    print(\"Downloading \"+file_testa)\n",
    "    urllib.request.urlretrieve(url+file_testa, file_testa)\n",
    "\n",
    "if not Path(file_testb).is_file():\n",
    "    print(\"Downloading \"+file_testb)\n",
    "    urllib.request.urlretrieve(url+file_testb, file_testb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Download Glove embeddings and unzip, if not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Glove Word Embeddings\n",
    "file = \"glove.6B.zip\"\n",
    "if not Path(\"glove.6B.zip\").is_file():\n",
    "    url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "    print(\"Start downoading Glove Word Embeddings. It will take some time, please wait...\")\n",
    "    urllib.request.urlretrieve(url, \"glove.6B.zip\")\n",
    "    print(\"Downloading finished\")\n",
    "else:\n",
    "    print(\"Glove data present.\")\n",
    "    \n",
    "if not Path(\"glove.6B.100d.txt\").is_file():\n",
    "    zip_ref = zipfile.ZipFile(file, 'r')\n",
    "    zip_ref.extractall(\"./\")\n",
    "    zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Create the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DL-NER\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\",\"8G\")\\\n",
    "    .config(\"spark.jars.packages\", \"JohnSnowLabs:spark-nlp:1.8.2\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"500m\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Load parquet dataset and cache into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|      conll_document|      conll_sentence|         conll_token|           conll_pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|EU rejects German...|[[document, 0, 27...|[[document, 0, 47...|[[token, 0, 1, EU...|[[pos, 0, 1, NNP,...|[[named_entity, 0...|\n",
      "|Rare Hendrix song...|[[document, 0, 96...|[[document, 0, 50...|[[token, 0, 3, Ra...|[[pos, 0, 3, NNP,...|[[named_entity, 0...|\n",
      "|China says Taiwan...|[[document, 0, 13...|[[document, 0, 46...|[[token, 0, 4, Ch...|[[pos, 0, 4, NNP,...|[[named_entity, 0...|\n",
      "|China says time r...|[[document, 0, 43...|[[document, 0, 39...|[[token, 0, 4, Ch...|[[pos, 0, 4, NNP,...|[[named_entity, 0...|\n",
      "|German July car r...|[[document, 0, 12...|[[document, 0, 50...|[[token, 0, 5, Ge...|[[pos, 0, 5, NNP,...|[[named_entity, 0...|\n",
      "|GREEK SOCIALISTS ...|[[document, 0, 44...|[[document, 0, 54...|[[token, 0, 4, GR...|[[pos, 0, 4, JJ, ...|[[named_entity, 0...|\n",
      "|BayerVB sets C$ 1...|[[document, 0, 63...|[[document, 0, 42...|[[token, 0, 6, Ba...|[[pos, 0, 6, NN, ...|[[named_entity, 0...|\n",
      "|Venantius sets $ ...|[[document, 0, 67...|[[document, 0, 46...|[[token, 0, 8, Ve...|[[pos, 0, 8, JJ, ...|[[named_entity, 0...|\n",
      "|Port conditions u...|[[document, 0, 18...|[[document, 0, 49...|[[token, 0, 3, Po...|[[pos, 0, 3, NN, ...|[[named_entity, 0...|\n",
      "|Israel plays down...|[[document, 0, 26...|[[document, 0, 42...|[[token, 0, 5, Is...|[[pos, 0, 5, NNP,...|[[named_entity, 0...|\n",
      "|Polish diplomat d...|[[document, 0, 82...|[[document, 0, 48...|[[token, 0, 5, Po...|[[pos, 0, 5, JJ, ...|[[named_entity, 0...|\n",
      "|Two Iranian oppos...|[[document, 0, 18...|[[document, 0, 47...|[[token, 0, 2, Tw...|[[pos, 0, 2, CD, ...|[[named_entity, 0...|\n",
      "|Saudi riyal rates...|[[document, 0, 58...|[[document, 0, 47...|[[token, 0, 4, Sa...|[[pos, 0, 4, JJ, ...|[[named_entity, 0...|\n",
      "|Israel approves A...|[[document, 0, 11...|[[document, 0, 46...|[[token, 0, 5, Is...|[[pos, 0, 5, NNP,...|[[named_entity, 0...|\n",
      "|Arafat to meet Pe...|[[document, 0, 80...|[[document, 0, 46...|[[token, 0, 5, Ar...|[[pos, 0, 5, NN, ...|[[named_entity, 0...|\n",
      "|Afghan UAE embass...|[[document, 0, 21...|[[document, 0, 50...|[[token, 0, 5, Af...|[[pos, 0, 5, NNP,...|[[named_entity, 0...|\n",
      "|Iraq 's Saddam me...|[[document, 0, 12...|[[document, 0, 43...|[[token, 0, 3, Ir...|[[pos, 0, 3, NNP,...|[[named_entity, 0...|\n",
      "|PRESS DIGEST - Ir...|[[document, 0, 52...|[[document, 0, 29...|[[token, 0, 4, PR...|[[pos, 0, 4, NNS,...|[[named_entity, 0...|\n",
      "|PRESS DIGEST - Le...|[[document, 0, 86...|[[document, 0, 32...|[[token, 0, 4, PR...|[[pos, 0, 4, NNS,...|[[named_entity, 0...|\n",
      "|CME live and feed...|[[document, 0, 61...|[[document, 0, 45...|[[token, 0, 2, CM...|[[pos, 0, 2, NNP,...|[[named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.training import CoNLL\n",
    "\n",
    "conll = CoNLL(\n",
    "    documentCol=\"conll_document\",\n",
    "    sentenceCol=\"conll_sentence\",\n",
    "    tokenCol=\"conll_token\",\n",
    "    posCol=\"conll_pos\"\n",
    ")\n",
    "\n",
    "training_data = conll.readDataset('file:///home/saif/Downloads/eng.train')\n",
    "training_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Create annotator components with appropriate params and in the right order. The finisher will output only NER. Put everything in Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence = SentenceDetector()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentence')\n",
    "\n",
    "token = Tokenizer()\\\n",
    "    .setInputCols(['sentence'])\\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "glove = WordEmbeddings()\\\n",
    "  .setInputCols([\"sentence\", \"token\"])\\\n",
    "  .setOutputCol(\"glove\")\\\n",
    "  .setEmbeddingsSource(\"file:///home/saif/Downloads/glove.6B.100d.txt\", 100, 2)\n",
    "\n",
    "nerTagger = NerDLApproach()\\\n",
    "  .setInputCols([\"sentence\", \"token\", \"glove\"])\\\n",
    "  .setTrainingCols(['conll_sentence', 'conll_token', 'glove'])\\\n",
    "  .setLabelColumn(\"label\")\\\n",
    "  .setOutputCol(\"ner\")\\\n",
    "  .setMaxEpochs(10)\\\n",
    "  .setRandomSeed(0)\\\n",
    "  .setVerbose(0)\n",
    "\n",
    "converter = NerConverter()\\\n",
    "  .setInputCols([\"document\", \"token\", \"ner\"])\\\n",
    "  .setOutputCol(\"ner_span\")\n",
    "    \n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"ner_span\"]) \\\n",
    "    .setIncludeMetadata(True)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "    document,\n",
    "    sentence,\n",
    "    token,\n",
    "    glove,\n",
    "    nerTagger,\n",
    "    converter,\n",
    "    finisher\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Train the pipeline. (This will take some time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fitting\n",
      "Fitting is ended\n",
      "649.7187893390656\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"Start fitting\")\n",
    "model = pipeline.fit(training_data)\n",
    "print(\"Fitting is ended\")\n",
    "print (time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Lets predict with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|Germany is a nice...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_data = spark.createDataFrame([[\"Germany is a nice place\"]]).toDF(\"text\")\n",
    "prediction_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+--------------------------+\n",
      "|                text|finished_ner_span|finished_ner_span_metadata|\n",
      "+--------------------+-----------------+--------------------------+\n",
      "|Germany is a nice...|               []|                        []|\n",
      "+--------------------+-----------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(prediction_data).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Save both pipeline and single model once trained, on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.write().overwrite().save(\"./ner_dl_pipeline\")\n",
    "model.write().overwrite().save(\"./ner_dl_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Load both again, deserialize from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel, Pipeline\n",
    "\n",
    "Pipeline.read().load(\"./ner_dl_pipeline\")\n",
    "sameModel = PipelineModel.read().load(\"./ner_dl_model\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
