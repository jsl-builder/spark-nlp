{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train POS Tagger in French by Spark NLP\n",
    "### Based on Universal Dependency `UD_French-GSD` version 2.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark `2.4` and Spark NLP `2.0.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "import time\n",
    "\n",
    "#Spark ML and SQL\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.sql.functions import array_contains\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "#Spark NLP\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import RegexRule\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a Spark Session for our app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Training_Perceptron\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\",\"6G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\")\\\n",
    "    .config(\"spark.jars\", \"/tmp/sparknlp.jar\")\\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/tmp/sparknlp.jar\")\\\n",
    "    .config(\"spark.executor.extraClassPath\", \"/tmp/sparknlp.jar\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"500m\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare our training datasets containing `token_posTag` like `de_DET`. You can download this data set from Amazon S3:\n",
    "\n",
    "```\n",
    "wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/fr/pos/UD_French/UD_French-GSD_2.3.txt -P /tmp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading UD_French-GSD_2.3.txt\n"
     ]
    }
   ],
   "source": [
    "# Download CoNLL-U French-GSD already converted to token_tag\n",
    "# Download CoNLL 2003 Dataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "url = 'https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/fr/pos/UD_French/'\n",
    "file_train='UD_French-GSD_2.3.txt'\n",
    "full_path='/tmp/'+file_train\n",
    "\n",
    "if not Path(full_path).is_file():   \n",
    "    print('Downloading '+file_train)\n",
    "    urllib.request.urlretrieve(url+file_train, full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.training import POS\n",
    "training_data = POS().readDataset(spark, '/tmp/UD_French-GSD_2.3.txt', '_', 'tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                tags|                text|\n",
      "+--------------------+--------------------+\n",
      "|[[pos, 0, 2, DET,...|Les commotions cé...|\n",
      "|[[pos, 0, 1, DET,...|L' œuvre est situ...|\n",
      "|[[pos, 0, 1, DET,...|Le comportement d...|\n",
      "|[[pos, 0, 8, ADV,...|Toutefois , les f...|\n",
      "|[[pos, 0, 5, PROP...|Ismene entre et a...|\n",
      "|[[pos, 0, 1, PRON...|je reviendrais av...|\n",
      "|[[pos, 0, 2, DET,...|Les forfaits comp...|\n",
      "|[[pos, 0, 1, PRON...|Il prévient que d...|\n",
      "|[[pos, 0, 2, PRON...|Ils tiraient à ba...|\n",
      "|[[pos, 0, 1, DET,...|Le château est en...|\n",
      "|[[pos, 0, 1, ADP,...|En effet , la bir...|\n",
      "|[[pos, 0, 1, DET,...|Le point final de...|\n",
      "|[[pos, 0, 1, DET,...|L' information gé...|\n",
      "|[[pos, 0, 5, VERB...|Motivé par la cha...|\n",
      "|[[pos, 0, 1, PRON...|Il exploitait un ...|\n",
      "|[[pos, 0, 3, ADV,...|Plus tard dans la...|\n",
      "|[[pos, 0, 2, PRON...|Ils deviennent al...|\n",
      "|[[pos, 0, 1, DET,...|Le chevalier lui ...|\n",
      "|[[pos, 0, 4, VERB...|Créée au cours du...|\n",
      "|[[pos, 0, 1, PRON...|On ne peut éviter...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\")\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\\\n",
    "    .addInfixPattern(\"(\\\\w+)([^\\\\s\\\\p{L}]{1})+(\\\\w+)\")\\\n",
    "    .addInfixPattern(\"(\\\\w+'{1})(\\\\w+)\")\\\n",
    "    .addInfixPattern(\"(\\\\p{L}+)(n't\\\\b)\")\\\n",
    "    .addInfixPattern(\"((?:\\\\p{L}\\\\.)+)\")\\\n",
    "    .addInfixPattern(\"([\\\\$#]?\\\\d+(?:[^\\\\s\\\\d]{1}\\\\d+)*)\")\\\n",
    "    .addInfixPattern(\"([\\\\p{L}\\\\w]+)\")\n",
    "\n",
    "posTagger = PerceptronApproach() \\\n",
    "    .setNIterations(6) \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"pos\") \\\n",
    "    .setPosCol(\"tags\")\n",
    "    \n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler, \n",
    "    sentence_detector, \n",
    "    tokenizer,\n",
    "    posTagger\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.2 ms, sys: 12.3 ms, total: 46.5 ms\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Let's train our Pipeline by using our training dataset\n",
    "model = pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our testing DataFrame where we get some sentences in French. We are going to use our trained Pipeline to transform these sentence and predict each token's `Part Of Speech`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest = spark.createDataFrame([\n",
    "    \"Je sens qu'entre ça et les films de médecins et scientifiques fous que nous avons déjà vus, nous pourrions emprunter un autre chemin pour l'origine.\",\n",
    "    \"On pourra toujours parler à propos d'Averroès de décentrement du Sujet.\"\n",
    "], StringType()).toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.transform(dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              result|              result|\n",
      "+--------------------+--------------------+\n",
      "|[Je, sens, qu', e...|[PRON, NOUN, PRON...|\n",
      "|[On, pourra, touj...|[PRON, VERB, ADV,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict.select(\"token.result\", \"pos.result\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
