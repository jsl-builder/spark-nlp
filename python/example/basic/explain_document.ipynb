{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use pretrained `explain_document` Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stages\n",
    "\n",
    " * DocumentAssembler\n",
    " * SentenceDetector\n",
    " * Tokenizer\n",
    " * Lemmatizer\n",
    " * Stemmer\n",
    " * Part of Speech\n",
    " * SpellChecker (Norvig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "#Spark ML and SQL\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.sql.functions import array_contains\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "#Spark NLP\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import RegexRule\n",
    "from sparknlp.base import DocumentAssembler, Finisher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a Spark Session for our app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Explain_Document\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\",\"8G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\")\\\n",
    "    .config(\"spark.jars\", \"/tmp/sparknlp.jar\")\\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/tmp/sparknlp.jar\")\\\n",
    "    .config(\"spark.executor.extraClassPath\", \"/tmp/sparknlp.jar\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"500m\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is our testing document, we'll use it to exemplify all different pipeline stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDoc = [\n",
    "\"French author who helped pioner the science-fiction genre. \\\n",
    "Verne wrate about space, air, and underwater travel before \\\n",
    "navigable aircrast and practical submarines were invented, \\\n",
    "and before any means of space travel had been devised. \"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = PipelineModel.load(\"../demo_pipelines/explain_document_ml_en_1.8.0_2.4_1552689539570\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are not interested in handling big datasets, let's switch to LightPipelines for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import LightPipeline\n",
    "lp = LightPipeline(pipeline)\n",
    "result = lp.annotate(testDoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's analyze these results - first let's see what sentences we detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['French author who helped pioner the science-fiction genre.',\n",
       "  'Verne wrate about space, air, and underwater travel before navigable aircrast and practical submarines were invented, and before any means of space travel had been devised.']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[content['sentence'] for content in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's see how those sentences were tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['French',\n",
       "  'author',\n",
       "  'who',\n",
       "  'helped',\n",
       "  'pioner',\n",
       "  'the',\n",
       "  'science-fiction',\n",
       "  'genre',\n",
       "  '.',\n",
       "  'Verne',\n",
       "  'wrate',\n",
       "  'about',\n",
       "  'space',\n",
       "  ',',\n",
       "  'air',\n",
       "  ',',\n",
       "  'and',\n",
       "  'underwater',\n",
       "  'travel',\n",
       "  'before',\n",
       "  'navigable',\n",
       "  'aircrast',\n",
       "  'and',\n",
       "  'practical',\n",
       "  'submarines',\n",
       "  'were',\n",
       "  'invented',\n",
       "  ',',\n",
       "  'and',\n",
       "  'before',\n",
       "  'any',\n",
       "  'means',\n",
       "  'of',\n",
       "  'space',\n",
       "  'travel',\n",
       "  'had',\n",
       "  'been',\n",
       "  'devised',\n",
       "  '.']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[content['token'] for content in result]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice some spelling errors? the pipeline takes care of that as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['French',\n",
       "  'author',\n",
       "  'who',\n",
       "  'helped',\n",
       "  'pioneer',\n",
       "  'the',\n",
       "  'science-fiction',\n",
       "  'genre',\n",
       "  '.',\n",
       "  'Verne',\n",
       "  'wrote',\n",
       "  'about',\n",
       "  'space',\n",
       "  ',',\n",
       "  'air',\n",
       "  ',',\n",
       "  'and',\n",
       "  'underwater',\n",
       "  'travel',\n",
       "  'before',\n",
       "  'navigable',\n",
       "  'aircraft',\n",
       "  'and',\n",
       "  'practical',\n",
       "  'submarines',\n",
       "  'were',\n",
       "  'invented',\n",
       "  ',',\n",
       "  'and',\n",
       "  'before',\n",
       "  'any',\n",
       "  'means',\n",
       "  'of',\n",
       "  'space',\n",
       "  'travel',\n",
       "  'had',\n",
       "  'been',\n",
       "  'devised',\n",
       "  '.']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[content['spell'] for content in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's see the lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['French',\n",
       "  'author',\n",
       "  'who',\n",
       "  'help',\n",
       "  'pioneer',\n",
       "  'the',\n",
       "  'science-fiction',\n",
       "  'genre',\n",
       "  '.',\n",
       "  'Verne',\n",
       "  'write',\n",
       "  'about',\n",
       "  'space',\n",
       "  ',',\n",
       "  'air',\n",
       "  ',',\n",
       "  'and',\n",
       "  'underwater',\n",
       "  'travel',\n",
       "  'before',\n",
       "  'navigable',\n",
       "  'aircraft',\n",
       "  'and',\n",
       "  'practical',\n",
       "  'submarine',\n",
       "  'be',\n",
       "  'invent',\n",
       "  ',',\n",
       "  'and',\n",
       "  'before',\n",
       "  'any',\n",
       "  'mean',\n",
       "  'of',\n",
       "  'space',\n",
       "  'travel',\n",
       "  'have',\n",
       "  'be',\n",
       "  'devise',\n",
       "  '.']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[content['lemmas'] for content in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check the stems, any difference with the lemmas shown bebore?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['french',\n",
       "  'author',\n",
       "  'who',\n",
       "  'help',\n",
       "  'pioneer',\n",
       "  'the',\n",
       "  'science-fict',\n",
       "  'genr',\n",
       "  '.',\n",
       "  'vern',\n",
       "  'wrote',\n",
       "  'about',\n",
       "  'space',\n",
       "  ',',\n",
       "  'air',\n",
       "  ',',\n",
       "  'and',\n",
       "  'underwat',\n",
       "  'travel',\n",
       "  'befor',\n",
       "  'navig',\n",
       "  'aircraft',\n",
       "  'and',\n",
       "  'practic',\n",
       "  'submarin',\n",
       "  'were',\n",
       "  'invent',\n",
       "  ',',\n",
       "  'and',\n",
       "  'befor',\n",
       "  'ani',\n",
       "  'mean',\n",
       "  'of',\n",
       "  'space',\n",
       "  'travel',\n",
       "  'had',\n",
       "  'been',\n",
       "  'devis',\n",
       "  '.']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[content['stems'] for content in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now it's the turn of Part Of Speech(POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('French', 'JJ'),\n",
       " ('author', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('helped', 'VBD'),\n",
       " ('pioner', 'NN'),\n",
       " ('the', 'DT'),\n",
       " ('science-fiction', 'NN'),\n",
       " ('genre', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Verne', 'NNP'),\n",
       " ('wrate', 'VBD'),\n",
       " ('about', 'IN'),\n",
       " ('space', 'NN'),\n",
       " (',', ','),\n",
       " ('air', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('underwater', 'JJ'),\n",
       " ('travel', 'NN'),\n",
       " ('before', 'IN'),\n",
       " ('navigable', 'JJ'),\n",
       " ('aircrast', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('practical', 'JJ'),\n",
       " ('submarines', 'NNS'),\n",
       " ('were', 'VBD'),\n",
       " ('invented', 'VBN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('before', 'IN'),\n",
       " ('any', 'DT'),\n",
       " ('means', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('space', 'NN'),\n",
       " ('travel', 'NN'),\n",
       " ('had', 'VBD'),\n",
       " ('been', 'VBN'),\n",
       " ('devised', 'VBN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = [content['pos'] for content in result]\n",
    "token = [content['token'] for content in result]\n",
    "# let's put token and tag together\n",
    "list(zip(token[0], pos[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
