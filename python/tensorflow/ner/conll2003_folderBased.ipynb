{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for training NER for CoNL2003\n",
    "Achives 91.5-97.7 f1 measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dim = 100\n",
    "word_embeddings_folder = '/home/saif/Downloads/'\n",
    "word_embeddings_file = word_embeddings_folder + 'glove.6B.{}d.txt'.format(word_dim)\n",
    "\n",
    "dataset_folder = '/home/saif/Downloads/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from ner_model import NerModel\n",
    "from dataset_encoder import DatasetEncoder\n",
    "from ner_model_saver import NerModelSaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove(file):\n",
    "    word2id = {}\n",
    "    vectors = []\n",
    "    \n",
    "    def add_vector(word, vector):\n",
    "        vectors.append(vector)\n",
    "        word2id[word] = len(word2id)        \n",
    "    \n",
    "    dummy_added = False\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            items = line.split(' ')\n",
    "            word = items[0]\n",
    "            vector = [float(x) for x in items[1:]]\n",
    "            \n",
    "            if not dummy_added:\n",
    "                add_vector('__oov__', [0] * len(vector))\n",
    "                dummy_added = True\n",
    "            \n",
    "            add_vector(DatasetEncoder.normalize(word), vector)\n",
    "    \n",
    "    return word2id, vectors\n",
    "\n",
    "\n",
    "# Returns sentences, each sentence is an array of tuples: (word, tag)\n",
    "def read_conll(folder):\n",
    "    \n",
    "    # array of tuple (word, tag)\n",
    "    sentence = []\n",
    "    \n",
    "    for root, subdirs, files in os.walk(folder):\n",
    "        #print(\"root is: \" + str(root))\n",
    "        #print(\"subdirs are: \" + str(subdirs))\n",
    "        #print(\"files are: \" + str(files))\n",
    "        for file in files:\n",
    "            curfile = os.path.join(root, file)\n",
    "            #print(\"parsing conll file: \" + str(curfile))\n",
    "            with open(curfile) as f:\n",
    "                for line in f:                \n",
    "                    items = line.strip().split(' ')\n",
    "                    if len(items) < 4 or items[0] == '-DOCSTART-':\n",
    "                        if len(sentence) > 0:\n",
    "                            yield sentence\n",
    "                            \n",
    "                            sentence = []\n",
    "                    else:\n",
    "                        word = items[0]\n",
    "                        tag = items[3]\n",
    "\n",
    "                        sentence.append((word, tag))\n",
    "                \n",
    "    if len(sentence) > 0:\n",
    "        yield sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xe2 in position 5454: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e0faca72e7f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_glove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embeddings_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-0b183ec22804>\u001b[0m in \u001b[0;36mread_glove\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdummy_added\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/encodings/ascii.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascii_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xe2 in position 5454: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "word2id, embeddings = read_glove(word_embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-38edbf430b2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'word2id' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(word2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = DatasetEncoder(word2id, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(encoder.encode(read_conll(dataset_folder)))\n",
    "valid = train\n",
    "test = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_in_embeddings(dataset):\n",
    "    zero = 0\n",
    "    other = 0\n",
    "    for sentence in dataset:\n",
    "        for word_id in sentence[\"word_ids\"]:\n",
    "            if word_id == 0:\n",
    "                zero += 1\n",
    "            else:\n",
    "                other += 1\n",
    "    \n",
    "    return (zero, other)\n",
    "\n",
    "(zero, other) = words_in_embeddings(train)\n",
    "print('train word embeddings coverage: {}'.format(other / (zero + other)))\n",
    "\n",
    "(zero, other) = words_in_embeddings(valid)\n",
    "print('valid word embeddings coverage: {}'.format(other / (zero + other)))\n",
    "\n",
    "(zero, other) = words_in_embeddings(test)\n",
    "print('test word embeddings coverage: {}'.format(other / (zero + other)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(encoder.encode(read_conll(dataset_folder)))))\n",
    "\n",
    "labels = set()\n",
    "for item in read_conll(dataset_folder):\n",
    "    labels  = labels | set(([label for (word, label) in item]))\n",
    "    \n",
    "    \n",
    "print(labels)\n",
    "print('chars: {}'.format(len(encoder.char2id)))\n",
    "\n",
    "all_chars = set()\n",
    "for item in read_conll(dataset_folder):\n",
    "    for (word, label) in item:\n",
    "        all_chars = all_chars | set(word)\n",
    "    \n",
    "print('chars: {}'.format(len(all_chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = NerModel()\n",
    "ner.add_cnn_char_repr(101, 25, 30)\n",
    "ner.add_pretrained_word_embeddings(word_dim)\n",
    "ner.add_context_repr(10, 200)\n",
    "ner.add_inference_layer(False)\n",
    "ner.add_training_op(5.0)\n",
    "\n",
    "ner.init_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, 1):\n",
    "    t = time.time()\n",
    "    ner.train(train, \n",
    "          valid, \n",
    "          lr = 0.2,\n",
    "          po = 0.05,\n",
    "          batch_size = 90,\n",
    "          dropout = 0.5, \n",
    "          epoch_start = i, \n",
    "          epoch_end = i + 1\n",
    "    )\n",
    "    (prec, rec, f1) = ner.measure(test)\n",
    "    print('Test quality prec: {}, rec: {}, f1: {}'.format(prec, rec, f1))\n",
    "    print(\"epoch time: \" + str(time.time() - t))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = NerModelSaver(ner, encoder, embeddings_file=word_embeddings_file)\n",
    "saver.save('conll_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
