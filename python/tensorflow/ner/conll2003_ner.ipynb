{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for training NER for CoNL2003\n",
    "Achives 93-96 f1 measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "\n",
    "from ner_model import NerModel\n",
    "from dataset_encoder import DatasetEncoder\n",
    "from ner_model_saver import NerModelSaver\n",
    "from embeddings_resolver import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dim = 1024 #1024  # 768, or 50, 100,  200, 300 for glove\n",
    "#word_dim = 100\n",
    "embeddings_type = 'bert' \n",
    "#embeddings_type = 'glove'\n",
    "\n",
    "glove_embeddings_file = 'glove.6B.{}d.txt'.format(word_dim)\n",
    "bert_embeddings_file = 'cased_L-24_H-1024_A-16' if word_dim == 1024 else 'cased_L-12_H-768_A-12'\n",
    "\n",
    "dataset_folder = './conll2003/'\n",
    "train_file = dataset_folder + 'eng.train'\n",
    "test_file_a = dataset_folder + 'eng.testa'\n",
    "test_file_b = dataset_folder + 'eng.testb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns sentences, each sentence is an array of tuples: (word, tag)\n",
    "def read_conll(file):\n",
    "    \n",
    "    # array of tuple (word, tag)\n",
    "    sentence = []\n",
    "    \n",
    "    with open(file) as f:\n",
    "        for line in f:                \n",
    "            items = line.strip().split(' ')\n",
    "            if len(items) < 4 or items[0] == '-DOCSTART-':\n",
    "                if len(sentence) > 0:\n",
    "                    yield sentence\n",
    "                    \n",
    "                    sentence = []\n",
    "            else:\n",
    "                word = items[0]\n",
    "                tag = items[3]\n",
    "                \n",
    "                sentence.append((word, tag))\n",
    "                \n",
    "    if len(sentence) > 0:\n",
    "        yield sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def slice(items, batch, start = 0):\n",
    "    result = []\n",
    "    for i, item in enumerate(items):\n",
    "        if i >= start:\n",
    "            result.append(item)\n",
    "            if len(result) >= batch:\n",
    "                yield result\n",
    "                result = []\n",
    "                \n",
    "    if result:\n",
    "        yield result\n",
    "            \n",
    "def read_dataset_incrementally(file, encoder, flush_size = 1000, embeddings_name = ''):\n",
    "    file_to_store = os.path.basename(file) + embeddings_name + '.hdf'\n",
    "    \n",
    "    if os.path.exists(file_to_store):\n",
    "        result = pd.read_hdf(file_to_store, key='dataset')\n",
    "        read = result.shape[0]\n",
    "    else:\n",
    "        result = pd.DataFrame()\n",
    "        read = 0\n",
    "        \n",
    "    sentences = read_conll(file)\n",
    "    \n",
    "    for sentences_batch in slice(sentences, flush_size, read):\n",
    "        encoded = encoder.encode(sentences_batch)\n",
    "        batch = pd.DataFrame(encoded)\n",
    "        result = pd.concat([result, batch])\n",
    "        result.to_hdf(file_to_store, key='dataset')\n",
    "        \n",
    "        print('totaly read:', result.shape[0])\n",
    "    \n",
    "    return result.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddings_name = '_' + embeddings_type + '_' + str(word_dim)\n",
    "\n",
    "if embeddings_type == 'glove':\n",
    "    resolver = EmbeddingsDbResolver.read_from_file(glove_embeddings_file, word_dim, lowercase=True)\n",
    "    encoder = DatasetEncoder(resolver)\n",
    "    train = read_dataset_incrementally(train_file, encoder, 5000, embeddings_name)\n",
    "    valid = read_dataset_incrementally(test_file_a, encoder, 5000, embeddings_name)\n",
    "    test = read_dataset_incrementally(test_file_b, encoder, 5000, embeddings_name)\n",
    "\n",
    "else:\n",
    "    bert = BertEmbeddingsResolver(bert_embeddings_file, max_length=128)\n",
    "    encoder = DatasetEncoder(bert)\n",
    "    \n",
    "    train = read_dataset_incrementally(train_file, encoder, 100, embeddings_name)\n",
    "    valid = read_dataset_incrementally(test_file_a, encoder, 100, embeddings_name)\n",
    "    test = read_dataset_incrementally(test_file_b, encoder, 100, embeddings_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_zero(arr):\n",
    "    for item in arr:\n",
    "        if item != 0.:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def words_in_embeddings(dataset):\n",
    "    zero = 0\n",
    "    other = 0\n",
    "    for sentence in dataset:\n",
    "        for embeddings in sentence[\"word_embeddings\"]:\n",
    "            if is_zero(embeddings):\n",
    "                zero += 1\n",
    "            else:\n",
    "                other += 1\n",
    "    \n",
    "    return (zero, other)\n",
    "\n",
    "(zero, other) = words_in_embeddings(train)\n",
    "print('train word embeddings coverage: {}'.format(other / (zero + other)))\n",
    "\n",
    "(zero, other) = words_in_embeddings(valid)\n",
    "print('valid word embeddings coverage: {}'.format(other / (zero + other)))\n",
    "\n",
    "(zero, other) = words_in_embeddings(test)\n",
    "print('test word embeddings coverage: {}'.format(other / (zero + other)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train))\n",
    "\n",
    "labels = set()\n",
    "for item in read_conll(train_file):\n",
    "    labels  = labels | set(([label for (word, label) in item]))\n",
    "    \n",
    "    \n",
    "print(labels)\n",
    "print('chars: {}'.format(len(encoder.char2id)))\n",
    "\n",
    "all_chars = set()\n",
    "for item in read_conll(train_file):\n",
    "    for (word, label) in item:\n",
    "        all_chars = all_chars | set(word)\n",
    "    \n",
    "print('chars: {}'.format(len(all_chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_tags = ['O', '[X]']\n",
    "dummy_tag_ids = [encoder.tag2id[tag] for tag in dummy_tags if tag in encoder.tag2id]\n",
    "\n",
    "ner = NerModel(dummy_tags = dummy_tag_ids, use_contrib=True)\n",
    "ner.add_cnn_char_repr(101, 25, 30)\n",
    "ner.add_bilstm_char_repr(101, 25, 30)\n",
    "ner.add_pretrained_word_embeddings(word_dim)\n",
    "ner.add_context_repr(10, 128, 3)\n",
    "ner.add_inference_layer(True)\n",
    "ner.add_training_op(5.0)\n",
    "\n",
    "ner.init_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, 100):\n",
    "    ner.train(train, \n",
    "          valid,\n",
    "          lr = 1e-3,\n",
    "          po = 0.005,\n",
    "          batch_size = 32,\n",
    "          dropout = 0.5,\n",
    "          epoch_start = i, \n",
    "          epoch_end = i + 1\n",
    "    )\n",
    "    (prec, rec, f1) = ner.measure(test)\n",
    "    print('Test quality prec: {}, rec: {}, f1: {}'.format(prec, rec, f1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = NerModelSaver(ner, encoder)\n",
    "saver.save('conll_model')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
