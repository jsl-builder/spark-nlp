{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for training NER for CoNL2003\n",
    "Achives 91.5-97.7 f1 measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dim = 100\n",
    "word_embeddings_folder = './'\n",
    "word_embeddings_file = word_embeddings_folder + 'glove.6B.{}d.txt'.format(word_dim)\n",
    "\n",
    "dataset_folder = './'\n",
    "train_file = dataset_folder + 'eng.train'\n",
    "test_file_a = dataset_folder + 'eng.testa'\n",
    "test_file_b = dataset_folder + 'eng.testb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "\n",
    "from ner_model import NerModel\n",
    "from dataset_encoder import DatasetEncoder\n",
    "from ner_model_saver import NerModelSaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove(file):\n",
    "    word2id = {}\n",
    "    vectors = []\n",
    "    \n",
    "    def add_vector(word, vector):\n",
    "        vectors.append(vector)\n",
    "        word2id[word] = len(word2id)        \n",
    "    \n",
    "    dummy_added = False\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        cnt = 0\n",
    "        for line in f:\n",
    "            items = line.split(' ')\n",
    "            word = items[0]\n",
    "            vector = [float(x) for x in items[1:]]\n",
    "            \n",
    "            if not dummy_added:\n",
    "                add_vector('__oov__', [0] * len(vector))\n",
    "                dummy_added = True\n",
    "            \n",
    "            add_vector(DatasetEncoder.normalize(word), vector)\n",
    "    \n",
    "    return word2id, vectors\n",
    "\n",
    "\n",
    "# Returns sentences, each sentence is an array of tuples: (word, tag)\n",
    "def read_conll(file):\n",
    "    \n",
    "    # array of tuple (word, tag)\n",
    "    sentence = []\n",
    "    \n",
    "    with open(file) as f:\n",
    "        for line in f:                \n",
    "            items = line.strip().split(' ')\n",
    "            if len(items) < 4 or items[0] == '-DOCSTART-':\n",
    "                if len(sentence) > 0:\n",
    "                    yield sentence\n",
    "                    \n",
    "                    sentence = []\n",
    "            else:\n",
    "                word = items[0]\n",
    "                tag = items[3]\n",
    "                \n",
    "                sentence.append((word, tag))\n",
    "                \n",
    "    if len(sentence) > 0:\n",
    "        yield sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id, embeddings = read_glove(word_embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = DatasetEncoder(word2id, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(encoder.encode(read_conll(train_file)))\n",
    "valid = list(encoder.encode(read_conll(test_file_a)))\n",
    "test = list(encoder.encode(read_conll(test_file_b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_in_embeddings(dataset):\n",
    "    zero = 0\n",
    "    other = 0\n",
    "    for sentence in dataset:\n",
    "        for word_id in sentence[\"word_ids\"]:\n",
    "            if word_id == 0:\n",
    "                zero += 1\n",
    "            else:\n",
    "                other += 1\n",
    "    \n",
    "    return (zero, other)\n",
    "\n",
    "(zero, other) = words_in_embeddings(train)\n",
    "print('train word embeddings coverage: {}'.format(other / (zero + other)))\n",
    "\n",
    "(zero, other) = words_in_embeddings(valid)\n",
    "print('valid word embeddings coverage: {}'.format(other / (zero + other)))\n",
    "\n",
    "(zero, other) = words_in_embeddings(test)\n",
    "print('test word embeddings coverage: {}'.format(other / (zero + other)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(encoder.encode(read_conll(train_file)))))\n",
    "\n",
    "labels = set()\n",
    "for item in read_conll(train_file):\n",
    "    labels  = labels | set(([label for (word, label) in item]))\n",
    "    \n",
    "    \n",
    "print(labels)\n",
    "print('chars: {}'.format(len(encoder.char2id)))\n",
    "\n",
    "all_chars = set()\n",
    "for item in read_conll(train_file):\n",
    "    for (word, label) in item:\n",
    "        all_chars = all_chars | set(word)\n",
    "    \n",
    "print('chars: {}'.format(len(all_chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = NerModel()\n",
    "ner.add_cnn_char_repr(101, 25, 30)\n",
    "ner.add_pretrained_word_embeddings(word_dim)\n",
    "ner.add_context_repr(10, 200)\n",
    "ner.add_inference_layer(False)\n",
    "ner.add_training_op(5.0)\n",
    "\n",
    "ner.init_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, 100):\n",
    "    ner.train(train, \n",
    "          valid, \n",
    "          lr = 0.2,\n",
    "          po = 0.05,\n",
    "          batch_size = 9,\n",
    "          dropout = 0.5, \n",
    "          epoch_start = i, \n",
    "          epoch_end = i + 1\n",
    "    )\n",
    "    (prec, rec, f1) = ner.measure(test)\n",
    "    print('Test quality prec: {}, rec: {}, f1: {}'.format(prec, rec, f1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = NerModelSaver(ner, encoder)\n",
    "saver.save('conll_model')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
