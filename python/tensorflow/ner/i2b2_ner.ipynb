{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for training i2b2 2010 dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "\n",
    "from ner_model import NerModel\n",
    "from dataset_encoder import DatasetEncoder\n",
    "from ner_model_saver import NerModelSaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_file = '/home/saif/Downloads/PubMed-shuffle-win-2.bin'\n",
    "i2b2_folder = '/home/saif/Downloads/i2b2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[56, 1, 64, 1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_proto = tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)\n",
    "list(config_proto.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.ASSET_FILEPATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns array of sentences, each contains array of tokens\n",
    "def read_texts(file):\n",
    "    with open(file, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split(' ')\n",
    "            yield words\n",
    "\n",
    "def read_concepts(file):\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            left, right = line.strip().split('||')\n",
    "            tokens = left.split(' ')\n",
    "            start = tokens[-2]\n",
    "            end = tokens[-1]\n",
    "            \n",
    "            start_line, start_token = [int(x) for x in start.split(':')]\n",
    "            end_line, end_token = [int(x) for x in end.split(':')]\n",
    "            assert(start_line == end_line)\n",
    "            line = start_line\n",
    "            \n",
    "            t, tag = right.split('=')\n",
    "            assert(t == 't')\n",
    "            tag = tag.strip('\"')           \n",
    "            \n",
    "            yield (line, start_token, end_token, tag)\n",
    "            \n",
    "\n",
    "# Iterator of sentences. Each sentence is an array of pairs (word, tag)\n",
    "def make_annotated_sentences(sentences, annotations):\n",
    "    tags = {}\n",
    "    \n",
    "    for (line, start_token, end_token, tag) in annotations:\n",
    "        for token in range(start_token, end_token + 1):\n",
    "            bio_tag = \"B-\" + tag if token == start_token else \"I-\" + tag\n",
    "            tags[(line, token)] = bio_tag\n",
    "    \n",
    "    line = 0\n",
    "    for sentence in sentences:\n",
    "        line += 1\n",
    "        result = []\n",
    "        \n",
    "        for i in range(len(sentence)):\n",
    "            token = sentence[i]\n",
    "            tag = tags.get((line, i), \"O\")\n",
    "            result.append((token, tag))\n",
    "        \n",
    "        yield result\n",
    "\n",
    "\n",
    "# Iterator of senteces, each sentence is an array of pairs (word, tag)\n",
    "def read_i2b2_dataset(folders):\n",
    "    \n",
    "    for folder in folders:\n",
    "        text_folder = folder + \"txt/\"\n",
    "        concept_folder = folder + \"concept/\"\n",
    "        \n",
    "        for file in os.listdir(text_folder):\n",
    "            if file[-4:] != \".txt\":\n",
    "                continue\n",
    "                \n",
    "            # remove txt\n",
    "            file = file[: -4]\n",
    "            text_file = text_folder + file + \".txt\"\n",
    "            concept_file = concept_folder +file + \".con\"\n",
    "            \n",
    "            sentences = read_texts(text_file)            \n",
    "            annotations = list(read_concepts(concept_file))\n",
    "            \n",
    "            for sentence in make_annotated_sentences(sentences, annotations):\n",
    "                yield sentence     \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Word Embeddings\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    embeddings_file, \n",
    "    binary=True,\n",
    "    limit=1000000)\n",
    "\n",
    "import collections\n",
    "normalize_tokens_for_embeddings = False\n",
    "#words = collections.OrderedDict({DatasetEncoder.normalize(w):w for w in model.vocab})\n",
    "words = collections.OrderedDict({w:w for w in model.vocab})\n",
    "\n",
    "vocab = list(words.keys())\n",
    "id2word = collections.OrderedDict({i+1: w for i,w in enumerate(vocab)})\n",
    "word2id = collections.OrderedDict({w:i for i,w in id2word.items()})\n",
    "\n",
    "def get_normalized_or_normal(target):\n",
    "    if normalize_tokens_for_embeddings:\n",
    "        try:\n",
    "            v = model.get_vector(DatasetEncoder.normalize(target))\n",
    "            v /= np.linalg.norm(v, 2)\n",
    "            return v\n",
    "        except KeyError:\n",
    "            v = model.get_vector(target)\n",
    "            v /= np.linalg.norm(v, 2)\n",
    "            return v\n",
    "    else:\n",
    "        return model.get_vector(target)\n",
    "\n",
    "embeddings = [[0]*200] + [get_normalized_or_normal(words[id2word[i]]) for i in range(1, len(words) + 1)]\n",
    "\n",
    "# Add word out of the vocabulary\n",
    "word2id['__oov__'] = 0\n",
    "id2word[0] = '__oov__'\n",
    "words['__oov__'] = '__oov__'\n",
    "\n",
    "# i2b2 reading\n",
    "train_dataset_folder = i2b2_folder + 'concept_assertion_relation_training_data/'\n",
    "sentences = read_i2b2_dataset([train_dataset_folder + \"beth/\", train_dataset_folder + \"partners/\"])\n",
    "train_dataset = list(sentences)\n",
    "\n",
    "valid_dataset_folder = i2b2_folder + 'reference_standard_for_test_data/'\n",
    "sentences = read_i2b2_dataset([valid_dataset_folder])\n",
    "valid_dataset = list(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00320693,  0.00167004, -0.09126581, -0.11574854, -0.04394112,\n",
       "       -0.07961337, -0.13876739,  0.03070446,  0.05947306, -0.01522299,\n",
       "       -0.09660824,  0.06576782, -0.22819473, -0.01563095, -0.03132185,\n",
       "       -0.05822439, -0.08672199,  0.1991438 , -0.05447187,  0.1072481 ,\n",
       "       -0.12158737, -0.04751258,  0.06938139,  0.01554571, -0.07477523,\n",
       "        0.05796184, -0.14733596,  0.10301121,  0.18611129,  0.14711392,\n",
       "       -0.02997275, -0.01465039, -0.06597033,  0.03484017,  0.10930625,\n",
       "       -0.12020653,  0.0046996 ,  0.12969127,  0.05813777,  0.07814306,\n",
       "       -0.04783545,  0.1214288 , -0.01741104, -0.10013006,  0.05751835,\n",
       "       -0.02224303,  0.10574778, -0.09843226,  0.07615267,  0.0214475 ,\n",
       "        0.0073724 ,  0.04157292,  0.04980931,  0.03333236, -0.06057598,\n",
       "        0.01574951,  0.06154851,  0.04370131, -0.05727746, -0.00469313,\n",
       "        0.0741053 , -0.09775556, -0.0806613 ,  0.06985603,  0.02253323,\n",
       "        0.029452  ,  0.02044853, -0.02627305, -0.02689816,  0.07067204,\n",
       "        0.0239744 ,  0.07170784, -0.07317017,  0.00050672,  0.02869161,\n",
       "        0.00368756, -0.05045789, -0.01308738, -0.11178124,  0.06871891,\n",
       "        0.0256869 ,  0.08397282, -0.0525538 , -0.04687524,  0.06289922,\n",
       "        0.0316439 , -0.02607769, -0.02801585,  0.0887232 ,  0.10467646,\n",
       "        0.03511443,  0.04683218,  0.04854683,  0.04311538,  0.02366187,\n",
       "        0.08708531,  0.05136274,  0.07101013,  0.01417876,  0.06714131,\n",
       "        0.05897265, -0.00995649,  0.0008968 , -0.05855122,  0.03661998,\n",
       "        0.06211822,  0.17039755,  0.01922642,  0.01887854,  0.10107052,\n",
       "        0.09758369,  0.02112313, -0.03432247, -0.01435866,  0.00106649,\n",
       "        0.07092029,  0.1260624 , -0.142397  ,  0.05716703,  0.0202684 ,\n",
       "       -0.10970776,  0.02383163,  0.07497239,  0.04292185,  0.10819909,\n",
       "        0.029831  , -0.01838652,  0.04378004,  0.00195238,  0.0762261 ,\n",
       "       -0.02410919,  0.00114508, -0.00688345,  0.01760098, -0.03329584,\n",
       "       -0.00753752, -0.02467156, -0.0494662 , -0.01755906, -0.10074002,\n",
       "        0.04043482, -0.01413293,  0.01967322,  0.09081233, -0.04229667,\n",
       "        0.04430403, -0.03267082,  0.08853558,  0.00136944, -0.24394321,\n",
       "       -0.03315664,  0.08777069, -0.02569037, -0.13970801, -0.04695432,\n",
       "        0.0897423 , -0.01274326, -0.01785786,  0.01107068,  0.02289459,\n",
       "        0.03446946,  0.03856229,  0.09319042,  0.07670508,  0.0175191 ,\n",
       "        0.00731042,  0.07664809, -0.0524    , -0.01705324,  0.06799756,\n",
       "       -0.06010545, -0.03392557, -0.01158063,  0.04591042, -0.11647902,\n",
       "        0.04481188,  0.0838557 ,  0.1793969 , -0.00300626, -0.05248716,\n",
       "       -0.0535149 ,  0.05399526, -0.02822259, -0.04760816,  0.0045098 ,\n",
       "       -0.01423226,  0.07393946, -0.06118452, -0.01355587,  0.00309191,\n",
       "        0.01423581,  0.00171058,  0.03761858, -0.08006135, -0.05681859,\n",
       "       -0.00896338, -0.04070131,  0.0477464 , -0.06790016, -0.06316665],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "v = model.get_vector(\"with\")\n",
    "v / np.linalg.norm(v, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-treatment', 'B-test', 'I-test', 'B-problem', 'O', 'I-problem', 'I-treatment'}\n"
     ]
    }
   ],
   "source": [
    "tags = set()\n",
    "\n",
    "for sentence in train_dataset:\n",
    "    for item in sentence:\n",
    "        tags.add(item[1])\n",
    "        \n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = DatasetEncoder(word2id, embeddings)\n",
    "train = list(encoder.encode(train_dataset))\n",
    "valid = list(encoder.encode(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words without embeddings coverage: 0.05923922396055457\n"
     ]
    }
   ],
   "source": [
    "def words_in_embeddings(dataset):\n",
    "    zero = 0\n",
    "    other = 0\n",
    "    for sentence in dataset:\n",
    "        for word_id in sentence[\"word_ids\"]:\n",
    "            if word_id == 0:\n",
    "                zero += 1\n",
    "            else:\n",
    "                other += 1\n",
    "    \n",
    "    return (zero, other)\n",
    "\n",
    "(zero, other) = words_in_embeddings(valid)\n",
    "print('words without embeddings coverage: {}'.format(zero / (zero + other)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saif/IdeaProjects/spark-nlp-models/python/tensorflow/ner/ner_model.py:127: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/saif/IdeaProjects/spark-nlp-models/python/tensorflow/ner/ner_model.py:128: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:430: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:454: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "ner = NerModel()\n",
    "\n",
    "ner.add_cnn_char_repr(dim=25, nfilters=30)\n",
    "ner.add_pretrained_word_embeddings(200)\n",
    "ner.add_context_repr(8, 200)\n",
    "ner.add_inference_layer(False)\n",
    "ner.add_training_op(5.0)\n",
    "\n",
    "ner.init_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, 110):\n",
    "    ner.train(train, \n",
    "          valid, \n",
    "          lr = 0.2,\n",
    "          po = 0.05,\n",
    "          batch_size = 180,\n",
    "          dropout = 0.6,\n",
    "          epoch_start = i, \n",
    "          epoch_end = i + 1\n",
    "    )\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        saver = NerModelSaver(ner, encoder, embeddings_file=embeddings_file)\n",
    "        saver.save('i2b2_model_non-normalized-drop_{}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner.predicted_labels.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "saver = NerModelSaver(ner, encoder, embeddings_file=embeddings_file)\n",
    "saver.save('i2b2_model')\n",
    "\n",
    "saver = NerModelSaver(ner, encoder, embeddings_file=embeddings_file)\n",
    "saver.save2('i2b2_asd')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.saved_model.loader.load(export_dir=\"i2b2_ss_model\", tags=['serve'], sess=ner.session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrics: prec: 0.9356028451833855, rec: 0.919586857701824, f1: 0.9275257178508727\n",
      "valid metrics: prec: 0.8451262784387393, rec: 0.8121615157920723, f1: 0.8283160495381372\n"
     ]
    }
   ],
   "source": [
    "NerModelSaver.restore_tensorflow_state(ner.session, 'i2b2_model_normalized_109')\n",
    "\n",
    "prec, rec, f1 = ner.measure(train)    \n",
    "print(\"train metrics: prec: {}, rec: {}, f1: {}\".format(prec, rec, f1))\n",
    "\n",
    "prec, rec, f1 = ner.measure(valid)    \n",
    "print(\"valid metrics: prec: {}, rec: {}, f1: {}\".format(prec, rec, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'char_repr/char_ids:0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.char_ids.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts tags in format BIO: B-\"tag\", I-\"tag\" to list with (begin, end, tag) tags\n",
    "def bio2be(source, tuples = False):\n",
    "    result = []\n",
    "    for i in range(len(source)):\n",
    "        sentence = source[i]\n",
    "        \n",
    "        last_start = None\n",
    "        last_tag = None\n",
    "        for j in range(len(sentence)):\n",
    "            tag = sentence[j]\n",
    "            if last_tag and (tag.startswith(\"B-\") or tag == \"O\"):\n",
    "                # close last tag\n",
    "                item = [i, last_start, j - 1, last_tag, '', '']\n",
    "                item = tuple(item) if tuples else item\n",
    "                result.append(item)\n",
    "                last_tag = None\n",
    "                last_start = None\n",
    "            \n",
    "            if tag.startswith(\"B-\") or (tag.startswith(\"I-\") and last_tag is None):\n",
    "                last_tag = tag[2:]\n",
    "                last_start = j\n",
    "                \n",
    "        if last_tag:\n",
    "            # close last tag in sentence\n",
    "            item = [i, last_start, len(sentence) - 1, last_tag, '', '']\n",
    "            item = tuple(item) if tuples else item\n",
    "            result.append(item)\n",
    "            last_tag = None\n",
    "            last_start = None\n",
    "\n",
    "    \n",
    "    return result                \n",
    "\n",
    "def decode_tags(id2tag, tag_ids):\n",
    "    result = []\n",
    "    for i in range(len(tag_ids)):\n",
    "        sentence = []\n",
    "        for j in range(len(tag_ids[i])):\n",
    "            tag_id = tag_ids[i][j]\n",
    "            sentence.append(id2tag[tag_id])\n",
    "        \n",
    "        result.append(sentence)\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def normalize_line(line):\n",
    "    return re.sub(r'[^\\w\\s$]',' ', line).strip()\n",
    "\n",
    "def read_test_dataset(file='benefit-summary.txt'):\n",
    "    with open(file) as f:\n",
    "        content = list([normalize_line(line) for line in f.readlines()])\n",
    "    return list([list([(word.strip(), \"unknown\") for word in line.split()]) for line in content])\n",
    "\n",
    "def read_test_lines(target):\n",
    "    content = list([normalize_line(line) for line in target])\n",
    "    return list([list([(word.strip(), \"unknown\") for word in line.split()]) for line in content])\n",
    "\n",
    "\n",
    "def save_dataset(dataset, file):\n",
    "    with open(file, 'w') as f:\n",
    "        for line in dataset:\n",
    "            words = list([word for (word, tag) in line])\n",
    "            f.write(' '.join(words))\n",
    "            f.write('\\n')\n",
    "\n",
    "def save_prediction(prediction, file):\n",
    "    with open(file, 'w') as f:\n",
    "        f.write('{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format('line', 'start', 'end', 'tag', 'text', 'sentence'))\n",
    "        for item in prediction:\n",
    "            f.write('{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(item[0], item[1], item[2], item[3], item[4], item[5]))\n",
    "\n",
    "def add_text_for_tags(predictions, dataset):\n",
    "    for prediction in predictions:\n",
    "        line = prediction[0]\n",
    "        start = prediction[1]\n",
    "        end = prediction[2]\n",
    "\n",
    "        words = dataset[line]['words'][start:end + 1]\n",
    "        prediction[4] = ' '.join(words)\n",
    "        prediction[5] = ' '.join(dataset[line]['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_dataset = read_test_dataset()\n",
    "text_dataset = read_test_lines([\n",
    "    \"With regard to the patient 's chronic obstructive pulmonary disease , the patient 's respiratory status improved throughout the remainder of her hospital course .\"\n",
    "])\n",
    "dataset = list(encoder.encode(text_dataset, True))\n",
    "print(len(dataset[0]['char_ids']))\n",
    "\n",
    "predicted = ner.predict(dataset, 1, 0.7)   \n",
    "print(predicted)\n",
    "id2tag = {tag_id:tag for tag, tag_id in encoder.tag2id.items()}\n",
    "print(id2tag)\n",
    "tags_predicted = list(bio2be(decode_tags(id2tag, predicted)))\n",
    "add_text_for_tags(tags_predicted, dataset)\n",
    "\n",
    "save_dataset(text_dataset, 'clean_data.txt')\n",
    "save_prediction(tags_predicted, 'prediction_09.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
